---
title: Building a Loan Prediction App with Databricks and PySpark
author: Christian Garcia
pubDatetime: 2025-06-03T16:00:00Z
slug: loan-prediction-databricks
featured: true
draft: false
tags:
  - PySpark
  - Databricks
  - FRED
  - BLS
  - Data Engineering
description:
  I'm building a loan prediction model using real economic data (FRED, BLS) in Databricks with PySpark.
  This blog will document my process step-by-step as I learn the tools and build something real.
---

## Motivation

As someone looking to transition into data engineering, I wanted to build a portfolio project that demonstrates real-world skillsâ€”not just basic Python scripts. I found an open role that uses **Databricks, PySpark, Terraform, and GitLab**, and I wanted to showcase my ability to use those tools effectively.

I also wanted the project to be relevant. Small businesses often struggle to get loans, and I thought: *what if I could build a simple app that predicts loan approval using real economic indicators like unemployment and inflation?*

## Project Overview

Hereâ€™s what Iâ€™m building:

- A loan prediction model trained on real data
- Ingests macroeconomic data from **FRED** and **BLS**
- Uses **PySpark** for transformations and modeling inside **Databricks**
- Saves data in Delta format for analysis
- Hosted frontend on my own VPS so others can try it out

Iâ€™ll be skipping Terraform for now since Databricks Community Edition doesnâ€™t support the APIs Terraform needs. Instead, Iâ€™ll focus on getting things working manually firstâ€”then automate later if needed.

## Tools Iâ€™m Using

- ğŸ§  **Databricks Community Edition** â€” where I run my PySpark notebooks
- ğŸ”— **FRED API** â€” for unemployment and interest rate data
- ğŸ“Š **BLS API** â€” for CPI and labor force data
- ğŸ§¹ **PySpark** â€” for cleaning, joining, and transforming everything
- ğŸ§ª **MLlib** â€” for training a classifier to predict loan approvals
- ğŸŒ **FastAPI (soon)** â€” to serve predictions
- ğŸ–¥ï¸ **My VPS** â€” to host the app for demo purposes

## First Milestone: Ingesting FRED Data

I created a notebook in Databricks called `01_ingest_fred` and pulled monthly unemployment rate data (`UNRATE`) from FRED from 2019 to 2024. Here's a preview of the code:

```python
from fredapi import Fred
import pandas as pd

fred = Fred(api_key="my_api_key")
unemployment = fred.get_series('UNRATE', '2019-01-01', '2024-01-01')

df = unemployment.reset_index()
df.columns = ['date', 'unemployment_rate']
```

Then I converted it into a PySpark DataFrame and saved it as a Delta table for use later in model training.

```python
spark_df = spark.createDataFrame(df)
spark_df.write.format("delta").mode("overwrite").save("/tmp/fred/unrate")
```

This gave me my first raw data source in Delta format â€” a huge win.

## Whatâ€™s Next

Hereâ€™s what Iâ€™ll tackle next:
	â€¢	Pull in BLS data (e.g., CPI, employment stats)
	â€¢	Upload a loan dataset (maybe SBA data)
	â€¢	Join macro data with loan records
	â€¢	Train and evaluate a PySpark model
	â€¢	Build a simple frontend to use it

## Final Thoughts

This project is stretching meâ€”Iâ€™m new to Databricks and PySparkâ€”but itâ€™s already paying off. Iâ€™ve learned more by building this out than I would have by reading docs or doing coding challenges. If youâ€™re trying to learn data engineering, I highly recommend starting with a real-world dataset and building something end-to-end.

Iâ€™ll keep blogging as I make progress. Stay tuned.